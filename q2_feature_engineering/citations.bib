@article{TADA,
    author = {Sayyari, Erfan and Kawas, Ban and Mirarab, Siavash},
    title = "{TADA: phylogenetic augmentation of microbiome samples enhances phenotype classification}",
    journal = {Bioinformatics},
    volume = {35},
    number = {14},
    pages = {i31-i40},
    year = {2019},
    month = {07},
    abstract = "{Learning associations of traits with the microbial composition of a set of samples is a fundamental goal in microbiome studies. Recently, machine learning methods have been explored for this goal, with some promise. However, in comparison to other fields, microbiome data are high-dimensional and not abundant; leading to a high-dimensional low-sample-size under-determined system. Moreover, microbiome data are often unbalanced and biased. Given such training data, machine learning methods often fail to perform a classification task with sufficient accuracy. Lack of signal is especially problematic when classes are represented in an unbalanced way in the training data; with some classes under-represented. The presence of inter-correlations among subsets of observations further compounds these issues. As a result, machine learning methods have had only limited success in predicting many traits from microbiome. Data augmentation consists of building synthetic samples and adding them to the training data and is a technique that has proved helpful for many machine learning tasks.In this paper, we propose a new data augmentation technique for classifying phenotypes based on the microbiome. Our algorithm, called TADA, uses available data and a statistical generative model to create new samples augmenting existing ones, addressing issues of low-sample-size. In generating new samples, TADA takes into account phylogenetic relationships between microbial species. On two real datasets, we show that adding these synthetic samples to the training set improves the accuracy of downstream classification, especially when the training data have an unbalanced representation of classes.TADA is available at https://github.com/tada-alg/TADA.Supplementary data are available at Bioinformatics online.}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btz394},
    url = {https://doi.org/10.1093/bioinformatics/btz394},
    eprint = {http://oup.prod.sis.lan/bioinformatics/article-pdf/35/14/i31/28913867/btz394.pdf},
}
@article{rognes2016vsearch,
  title={VSEARCH: a versatile open source tool for metagenomics},
  author={Rognes, Torbj{\o}rn and Flouri, Tom{\'a}{\v{s}} and Nichols, Ben and Quince, Christopher and Mah{\'e}, Fr{\'e}d{\'e}ric},
  journal={PeerJ},
  volume={4},
  pages={e2584},
  year={2016},
  publisher={PeerJ Inc.},
  doi={10.7717/peerj.2584}
}
@article{ADASYN,
author={ {Haibo He} and {Yang Bai} and E. A. {Garcia} and {Shutao Li}},
booktitle={2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)},
title={ADASYN: Adaptive synthetic sampling approach for imbalanced learning},
year={2008},
volume={},
number={},
pages={1322-1328},
abstract={This paper presents a novel adaptive synthetic (ADASYN) sampling approach for learning from imbalanced data sets. The essential idea of ADASYN is to use a weighted distribution for different minority class examples according to their level of difficulty in learning, where more synthetic data is generated for minority class examples that are harder to learn compared to those minority examples that are easier to learn. As a result, the ADASYN approach improves learning with respect to the data distributions in two ways: (1) reducing the bias introduced by the class imbalance, and (2) adaptively shifting the classification decision boundary toward the difficult examples. Simulation analyses on several machine learning data sets show the effectiveness of this method across five evaluation metrics.},
keywords={learning (artificial intelligence);pattern classification;sampling methods;statistical distributions;adaptive synthetic sampling approach;imbalanced data set learning;weighted distribution;classification decision boundary;imbalanced data classification;Classification algorithms;Decision trees;Algorithm design and analysis;Training data;Machine learning;Accuracy;Machine learning algorithms},
doi={10.1109/IJCNN.2008.4633969},
ISSN={},
month={June},}
@article{chawla2002smote,
  title={SMOTE: synthetic minority over-sampling technique},
  author={Chawla, Nitesh V and Bowyer, Kevin W and Hall, Lawrence O and Kegelmeyer, W Philip},
  journal={Journal of artificial intelligence research},
  volume={16},
  pages={321--357},
  year={2002},
  abstract={An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of ``normal'' examples with only a small percentage of ``abnormal'' or ``interesting'' examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},	
}
@article{imblearn,
author  = {Guillaume  Lema{{\^i}}tre and Fernando Nogueira and Christos K. Aridas},
title   = {Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning},
journal = {Journal of Machine Learning Research},
year    = {2017},
volume  = {18},
number  = {17},
pages   = {1-5},
url     = {http://jmlr.org/papers/v18/16-365.html}
}
